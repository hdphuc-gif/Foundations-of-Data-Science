{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Science (GDW) 2023\n",
    "\n",
    "\n",
    "\n",
    "# Exercise IX: Entropy & Overfitting\n",
    "\n",
    "This week, we will take a look at entropy and the phenomenon of overfitting, especially for the model class of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Shannon Entropy\n",
    "\n",
    "Suppose we want to measure the amount of information in a selection process by a function $H$, the\n",
    "following properties look reasonable:\n",
    "- the function $H$ should be continuous,\n",
    "- if all elements are equaly likely $p_{element} = 1/n$, the function $H$ should increase monotonical with the number of possibilities $n$\n",
    "- if the selection consists of multiple steps, H should be the weighted sum.\n",
    "\n",
    "Following the proof from ..., the only possible function with these properties is the\n",
    "Shannon entropy \n",
    "\n",
    "$\\hspace{6cm} H = - \\sum\\limits_{x \\in X} p(x) \\log p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a look at an example:\n",
    "\n",
    "A fair die is rolled at the same time as a fair coin is tossed. Let A be the number on the upper\n",
    "surface of the die and let B describe the outcome of the coin toss, where B is equal to 1 if the result\n",
    "is “head” and it is equal to 0 if the result if “tail”. \n",
    "\n",
    "The random variables X and Y are given by $X = A + B$ and $Y = A − B$, respectively. Let $a, b, x$, and $y$ denote possible values of the random variables $A, B, X,$ and $Y$ , respectively.\n",
    "\n",
    "The random variable $X = A + B$ can take the values 1 to 7. The probability masses $p(x)$ for the\n",
    "values 1 and 7 are equal to 1/12, since they correspond to exactly one event. The probability masses\n",
    "for the values 2 to 6 are equal to 1/6, since each of these values corresponds to two events {a, b}.\n",
    "\n",
    "Similarly, the random variable $Y = A − B$ can take the values 0 to 6, where the probability masses\n",
    "for the values 0 and 6 are equal to 1/12, while the probability masses for the values 1 to 5 are equal\n",
    "to 1/6.\n",
    "\n",
    "The following tables list the possible values of the random variables $X$ and $Y$ , the associated events\n",
    "$\\{a, b\\}$, and the probability masses $p_X(x)$ and $p_Y(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | events {a,b} | p_X(x) |//| y | events {a,b} | p_Y(y) |\n",
    "|---|--------------|--------||---|--------------|--------|\n",
    "| 1 | {1,0}        | 1/12   || 0 | {1,1}        | 1/12   |\n",
    "| 2 | {2,0},{1,1}  | 1/6    || 1 | {1,0},{2,1}  | 1/6    |\n",
    "| 3 | {3,0}, {2,1} | 1/6    || 2 | {2,0}, {3,1} | 1/6    |\n",
    "| 4 | {4,0}, {3,1} | 1/6    || 3 | {3,0}, {4,1} | 1/6    |\n",
    "| 5 | {5,0}, {4,1} | 1/6    || 4 | {4,0}, {5,1} | 1/6    |\n",
    "| 6 | {6,0}, {5,1} | 1/6    || 5 | {5,0}, {6,1} | 1/6    |\n",
    "| 7 | {6,1}        | 1/12   || 6 | {6,0}        | 1/12   |\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "Calculate the entropies $H(X)$ and $H(Y)$, the conditional entropies $H(X|Y)$ and $H(Y|X)$, the joint\n",
    "entropy $H(X,Y)$ and the mutual information $I(X;Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*write your solution here or on paper*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Overfitting Trees\n",
    "\n",
    "Given the *iris* dataset, we can learn a classiﬁer on the ﬁrst two features with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2] # features\n",
    "y = iris.target # labels\n",
    "\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Specify Model\n",
    "iris_model = DecisionTreeRegressor(random_state=1, max_depth=None)\n",
    "# Fit Model\n",
    "iris_model.fit(train_X, train_y)\n",
    "\n",
    "# calculate mean absolute error on training instances\n",
    "train_predictions = iris_model.predict(train_X)\n",
    "train_mae = mean_absolute_error(train_predictions, train_y)\n",
    "print (\"Training MAE \" + repr(train_mae))\n",
    "\n",
    "# Make validation predictions and calculate mean absolute error\n",
    "val_predictions = iris_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print (\"Validation MAE \" + repr(val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you observe, the training MAE is much smaller than the validation MAE. This phenomenon is known as *overﬁtting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "Reduce the model complexity by changing the parameter `max_depth` and observe its behaviour.\n",
    "What do you think is the optimal tree depth, when comparing MAEs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib . pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_with_depth(X, y, maxdepth):\n",
    "\n",
    "    # Split into validation and training data\n",
    "    t_mae =[0]* maxdepth\n",
    "    v_mae =[0]* maxdepth\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X,y)\n",
    "\n",
    "    for depth in range (1, maxdepth+1):\n",
    "        # Specify Model\n",
    "        iris_model = DecisionTreeRegressor(max_depth = depth)\n",
    "        # Fit Model\n",
    "        iris_model.fit(train_X, train_y)\n",
    "        # calculate mean absolute error on training instances\n",
    "        train_predictions = iris_model.predict(train_X)\n",
    "        t_mae[depth-1] = mean_absolute_error(train_predictions, train_y)\n",
    "        # Make validation predictions and calculate mean absolute error\n",
    "        val_predictions = iris_model.predict(val_X)\n",
    "        v_mae[depth-1] = mean_absolute_error(val_predictions, val_y)\n",
    "        # print (\" Validation MAE \"+ repr ( val_mae ) )\n",
    "\n",
    "    t_mae = np.asarray(t_mae)\n",
    "    v_mae = np.asarray(v_mae)\n",
    "    return t_mae, v_mae\n",
    "    \n",
    "t_mae, v_mae = train_with_depth(X, y, maxdepth=10)\n",
    "\n",
    "plt.plot(range(1, maxdepth+1), t_mae, c=\"blue\")\n",
    "plt.plot(range(1, maxdepth+1), v_mae , c=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since we set no seed, if you rerun the code, you can see that the results diﬀer, as they are highly dependent on the split of training and validation data. \n",
    "This means, that we can not generalize our ﬁndings, but instead have to average over multiple\n",
    "random splits, to get the expected error values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "Repeat the code for at least 100 repetitions and take the average MAEs. Plot the results in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "Which is the prefered depth for our model? Define a variable `pref_depth=` that holds your chosen value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_depth=-1 # <- change this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may train and plot one of these trees by executing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "iris_model = DecisionTreeRegressor(max_depth=pref_depth)\n",
    "# Fit Model\n",
    "iris_model.fit(train_X, train_y)\n",
    "plt.figure(figsize =(10, 7))\n",
    "p = plot_tree(iris_model, label=\"none\", feature_names=[\"length\", \"width\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the leafs you can see the predicted label. As you realize the label is continuous, because we applied a re-\n",
    "gression model, but actually predicting a discrete class is a Classiﬁcation problem.\n",
    "\n",
    "You may train and plot a Classiﬁcation tree with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris_model = DecisionTreeClassifier(max_depth=pref_depth)\n",
    "# Fit Model\n",
    "iris_model.fit(train_X, train_y)\n",
    "plt.figure(figsize=(10, 7))\n",
    "p = plot_tree(iris_model, label=\"none\", impurity=True, filled=True, feature_names=[\"length\", \"width\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
